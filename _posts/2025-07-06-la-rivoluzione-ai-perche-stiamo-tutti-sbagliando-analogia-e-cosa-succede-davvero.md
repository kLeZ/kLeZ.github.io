---
title: "La Rivoluzione AI: Perché Stiamo Tutti Sbagliando Analogia (E Cosa Succede Davvero)"
tags:
- AI
- rivoluzione industriale
- analogie
- riflessioni
date: 2025-07-06 14:30:42+02:00
---

*Ovvero: come l'intelligenza artificiale sta riscrivendo le regole dell'economia più velocemente di quanto i nostri cervelli analogici riescano a capire (forse)*

Se dovessi fare un drink ogni volta che qualcuno paragona l'intelligenza artificiale alla rivoluzione industriale, a quest'ora sarei già morto per coma etilico. È diventato il paragone del momento, quel cliché che esce fuori a ogni conferenza TED, l'introduzione obbligatoria di praticamente ogni articolo che si rispetti sull'AI. "Proprio come la macchina a vapore ha cambiato tutto..." iniziano tutti, e io già so che nei prossimi dieci minuti sentirò variazioni sul tema che fanno sembrare ChatGPT l'equivalente digitale della prima locomotiva.

{% include more.html %}

(Per inciso, scrivo questo articolo nella speranza che almeno tre persone lo leggano fino in fondo senza addormentarsi. Se sei arrivato fin qui, congratulazioni: fai già parte di una minoranza statistica.)

Il problema è che questo paragone, per quanto rassicurante e di moda, mi sembra tanto generico quanto inutile. È un po' come dire che "mangiare è come nutrirsi" - tecnicamente vero, ma che ci dice? E soprattutto, ho la sensazione che ci stia facendo perdere di vista qualcosa di molto più interessante (e forse preoccupante) di quello che sta davvero succedendo.

Non che io abbia la verità in tasca, per carità. Ma dopo vent'anni che programmo e vedo come cambia questo settore (sì, sono uno di quelli che ha iniziato quando Java era ancora "rivoluzionario" e tutti dicevano che C++ era il futuro), alcune cose mi sembrano... strane. E forse vale la pena ragionarci su.

Un po' come quando vedi il tuo coinquilino che continua a mettere la pasta nell'acqua fredda e dici "mmh, forse c'è un modo migliore", ma poi realizzi che in realtà non sei sicuro neanche tu se stai facendo tutto giusto.

## Il teatrino della "quarta rivoluzione"

Ok, facciamo un passo indietro. L'idea standard che va per la maggiore è questa: abbiamo avuto la prima rivoluzione industriale con vapore e carbone, la seconda con elettricità e catene di montaggio, la terza con computer e internet, e ora eccoci qua alla quarta con AI e automazione. Tutto molto ordinato, molto progressivo. Una bella timeline da PowerPoint aziendale che fa sentire tutti intelligenti.

Gli economisti di mezzo mondo stanno sfornando paper su come "l'AI sostituirà i lavoratori proprio come fecero i telai meccanici con i tessitori", mentre i politici fanno i loro bei discorsi su come "dobbiamo prepararci al cambiamento come fecero i nostri antenati". Tutto molto rassicurante, tutto molto "ce l'abbiamo già fatta una volta, ce la faremo anche stavolta".

Ma ecco, io ho un dubbio. Anzi, più di uno.

La rivoluzione industriale classica ha preso **decenni** per dispiegarsi completamente. La ferrovia inglese non è mica spuntata fuori in tre anni, e i tessitori non sono stati rimpiazzati da un giorno all'altro. C'è stato tempo per adattarsi, per riqualificarsi, per inventare nuovi mestieri. Il tutto con un ritmo che, seppur drammatico per l'epoca, oggi ci sembrerebbe quasi glaciale.

L'AI, invece, mi sa che sta facendo tutto questo in **mesi**. ChatGPT è passato da zero a 100 milioni di utenti più velocemente di quanto io riesca a finire una serie Netflix (e non è che sono lento con Netflix). E mentre sto scrivendo queste righe, stanno uscendo nuovi modelli che rendono obsoleti quelli di sei mesi fa.

Forse sto esagerando, ma qualcosa non torna.

## Forse stiamo sbagliando analogia

Ecco dove secondo me tutti sbagliano analogia. Non dovremmo paragonare la programmazione al settore tessile - quello che è stato praticamente spazzato via dalla meccanizzazione. Dovremmo paragonarla alla lavorazione del legno, del ferro, della ceramica. O almeno, questa è la mia teoria.

Prendiamo IKEA. È un'azienda che ha industrializzato la produzione di mobili, eppure non ha mica ucciso l'artigianato del legno. Ha semplicemente creato due mercati paralleli: quello industriale (scaffali Billy da 29 euro che montiamo tutti bestemmiando contro le istruzioni in svedese) e quello artigianale (tavoli su misura che costano quanto una macchina usata e ti fanno sentire un riccone per averli ordinati). 

La differenza? IKEA ha investito una fortuna nella **progettazione a monte**. Hanno capito - o almeno così mi sembra - che il valore non stava nell'assemblaggio (che infatti fanno fare a noi poveri clienti), ma nel design del sistema che rende l'assemblaggio scalabile. È geniale, in un modo leggermente psicopatico.

Ecco dove penso stiamo andando con la programmazione. I programmatori di domani probabilmente non saranno quelli che scrivono righe di codice (quello lo farà l'AI), ma quelli che **progettano sistemi** che orchestrano l'AI. Un po' come i falegnami moderni che non segano tavole a mano, ma progettano pezzi e supervisionano macchine.

Il settore tessile, invece, è praticamente sparito. Quando l'ultima nonna ha smesso di fare calze ai ferri, non è rimasto granché di un "mercato premium delle calze artigianali". Perché? Forse perché fare calze non richiedeva creatività progettuale - era pura esecuzione ripetitiva.

Boh, magari sbaglio completamente. Ma mi sembra una distinzione importante.

## Il problema della velocità

Le rivoluzioni industriali precedenti avevano una cosa in comune: erano **energeticamente limitate**. Potevi costruire solo un numero finito di fabbriche, assumere solo un numero finito di operai, produrre solo una quantità finita di merci. I vincoli fisici imponevano un ritmo, volente o nolente.

L'AI, invece, vive nel mondo digitale dove i **vincoli fisici sono... be', opzionali**. Una volta addestrato un modello, puoi replicarlo all'infinito istantaneamente. Non hai bisogno di costruire nuove fabbriche - basta copiare il file. È come se, nel 1800, invece di dover costruire ogni singola macchina a vapore, bastasse dire "Ctrl+C, Ctrl+V" e apparissero magicamente cento locomotive identiche.

Questo crea un problema che, per quello che ne so, gli economisti non avevano mai visto prima: cicli di innovazione **compressi nel tempo** ma **amplificati in scala**. Invece di decenni di adattamento graduale, abbiamo anni (forse) di sconvolgimento totale.

E qui entra in gioco una teoria che, non so perché, nessuno sembra aver considerato: i **cicli inflattivi-deflattivi accelerati**. O almeno, non ne ho mai sentito parlare. Magari esiste già e io non lo so (se qualcuno dei miei tre lettori ha fonti, le accetto volentieri).

Piccola digressione: questo è uno di quei momenti in cui spero davvero di sbagliarmi, perché se ho ragione significa che stiamo tutti guidando verso un muro a 200 all'ora convinti di essere in autostrada. Ma andiamo avanti.

## I cicli a fisarmonica

Nelle rivoluzioni industriali classiche, i cicli economici erano lenti e abbastanza prevedibili. Prima una lunga fase "inflattiva" di investimenti massicci in infrastrutture (ferrovie, fabbriche, centrali elettriche), poi una fase "deflattiva" di ottimizzazione ed efficientamento che durava decenni.

Il boom delle ferrovie americane, per dire, è durato dal 1840 al 1890. Cinquant'anni di "buttiamo soldi nelle rotaie" seguiti da altrettanti anni di "ora ottimizziamo i treni". Ritmi glaciali, ma forse per questo sostenibili.

Con l'AI, invece, ho l'impressione che i cicli si stiano **comprimendo drasticamente**. Potremmo avere fasi inflattive di 2-3 anni (tutti a comprare GPU come se fossero diamanti) seguite da fasi deflattive altrettanto brevi (tutti a ottimizzare algoritmi per fare la stessa cosa con meno potenza).

E indovinate un po'? Mi sa che siamo proprio nel mezzo di questo casino.

## Perché non ci fermiamo?

Ora arriviamo alla parte che mi incuriosisce di più. Secondo la logica economica naturale - sempre che io la capisca qualcosa - dovremmo **già essere** in fase deflattiva. I modelli di AI generativa di base esistono, funzionano discretamente, e ora dovremmo concentrarci sull'ottimizzazione. Invece di continuare a pompare energia bruta nel sistema, dovremmo essere nella fase "facciamo meglio con meno".

Ma non sta succedendo. Perché? Ho un paio di teorie, ma magari sono completamente sbagliate.

**Primo motivo: il capitalismo non vuole frenare**. Nelle fasi inflattive, chi ha i soldi può moltiplicarli velocemente vendendo prodotti ad alto valore aggiunto. È il momento in cui i capitalisti fanno la pacchia. Nelle fasi deflattive, invece, vince chi fa ricerca - e la ricerca costa tanto, rende dopo anni, e non garantisce profitti immediati. 

Chi pensate che preferisca vendere servizi AI da centinaia di migliaia di dollari **oggi**, o investire tre anni in ricerca per renderli più efficienti **domani**? Mmh. È come chiedere a un drogato se preferisce la dose immediata o il programma di riabilitazione.

**Secondo motivo: la geopolitica**. L'AI sembra essere diventata la nuova "corsa allo spazio" della Guerra Fredda 2.0. USA e Cina si stanno sfidando a chi ha l'intelligenza artificiale più grossa, e presumibilmente nessuno vuole essere il primo a dire "Ok, fermiamoci a ottimizzare". È come una gara di muscoli dove il primo che si rilassa viene visto come debole.

Il risultato? Stiamo **artificialmente** mantenendo la fase inflattiva oltre il suo ciclo naturale, pompando risorse nella macchina invece di renderla più efficiente.

O almeno, questa è la mia impressione. Potrei sbagliarmi completamente.

## I chip specializzati che nessuno nota

Ma se guardate attentamente - e ammetto che potrei vedere cose che non ci sono - la fase deflattiva sembra già iniziata **sottotraccia**. Mentre tutti fanno rumore sulle GPU sempre più potenti, c'è un'intera industria che sta silenziosamente lavorando sull'efficientamento.

Groq sta sviluppando chip LPU progettati specificamente per elaborare linguaggio, non per la grafica. Google ha le sue TPU giunte alla sesta generazione. Amazon ha i suoi Trainium. Microsoft sta preparando Maia. Persino OpenAI sta collaborando con Broadcom per un chip custom.

Tutti questi chip hanno una cosa in comune: sono progettati per fare **la stessa cosa con meno energia**. Non per essere più potenti, ma per essere più efficienti. È esattamente quello che ti aspetteresti in una fase deflattiva.

Ma siccome il mercato è ancora in modalità "più grande = meglio", questi sviluppi passano in secondo piano rispetto alle GPU da mille watt che fanno più rumore mediatico.

Forse sto leggendo troppo in questi segnali. Ma mi sembrano interessanti.

## L'elefante quantistico

E poi c'è l'elefante nella stanza che tutti sembrano ignorare: il quantum computing.

Teoricamente, un computer quantistico potrebbe risolvere certi problemi di AI con una frazione dell'energia di un data center tradizionale. È l'efficientamento definitivo - come passare dalla macchina a vapore al motore elettrico. Teoricamente.

Indovinate chi ci sta lavorando? Gli stessi che stanno pompando miliardi in GPU tradizionali: Google, IBM, Microsoft. È come se stessero giocando su due tavoli contemporaneamente - uno pubblico dove fanno rumore con l'hardware bruto, uno privato dove preparano la rivoluzione dell'efficientamento.

Il fatto che ne parlino relativamente poco mi sembra indicativo. Se fossimo in fase naturale deflattiva, il quantum computing per AI dovrebbe essere al centro del dibattito. Invece è relegato a comunicati stampa di aziende specializzate e paper accademici che legge giusto qualche nerd come me (e probabilmente tu, se sei arrivato fin qui).

Ma potrei sbagliarmi. Magari il quantum computing per l'AI è ancora troppo acerbo per essere rilevante. O magari stanno tutti zitti perché sanno qualcosa che noi non sappiamo. Il che è sempre rassicurante.

## La contraddizione climatica

E qui arriviamo alla contraddizione che mi lascia più perplesso.

Siamo ufficialmente in crisi climatica. Ogni conferenza internazionale ci ricorda che dovremmo ridurre i consumi energetici. Parallelamente, stiamo costruendo data center che consumano come piccole nazioni per far girare modelli AI che spesso vengono usati per creare meme di gatti con sei zampe.

La logica vorrebbe che ci concentrassimo sull'efficientamento energetico dell'AI. Invece continuiamo a buttare energia nel sistema come se i watt fossero gratis e il pianeta fosse immortale.

Perché? La mia teoria è che fermarsi a ottimizzare significherebbe ammettere che la fase di crescita infinita è finita. E né i capitalisti né i governi sembrano pronti per quella conversazione.

Ma boh, magari sono io che vedo contraddizioni dove non ce ne sono.

## Il palloncino USA-Cina

Nel frattempo, USA e Cina continuano la loro pantomima della "supremazia AI", ognuno terrorizzato dall'idea che l'altro possa prendergli il posto di "egemone tecnologico mondiale".

È interessante notare che questa competizione sembra artificialmente **estendere** la fase inflattiva ben oltre il suo ciclo naturale. È come se due bambini facessero a gara a chi riesce a gonfiare di più un palloncino, ignorando il fatto che prima o poi scoppierà.

Gli americani bloccano l'export di chip avanzati verso la Cina. I cinesi sviluppano le loro alternative. Entrambi pompano miliardi in ricerca. Il risultato? Un'accelerazione artificiale che sta bruciando risorse a velocità industriale per mantenere una competizione che, dal punto di vista economico puro, forse dovrebbe già essere nella fase di consolidamento.

O almeno, così mi sembra. Magari è normale che funzioni così.

## Segnali sottotraccia

Ma i segnali che il ciclo naturale sta provando a riemergere mi sembrano esserci tutti:

- **Standardizzazione**: I modelli AI stanno convergendo verso architetture simili
- **Specializzazione hardware**: Chip custom per carichi di lavoro specifici
- **Focus sull'efficienza**: Algoritmi che fanno di più con meno parametri
- **Costi operativi**: Le aziende iniziano a calcolare quanto costa davvero far girare questi modelli

È la classica transizione da "più potenza bruta" a "più intelligenza nell'usare la potenza". Esattamente quello che ti aspetteresti quando una tecnologia matura.

Il problema è che questa transizione naturale sembra essere **combattuta** artificialmente da forze economiche e geopolitiche che hanno interesse a mantenere il boom il più a lungo possibile.

Ma ripeto, potrei sbagliarmi completamente su tutto.

## Le mie previsioni (probabilmente sbagliate)

Ecco la mia previsione, che potete tranquillamente usare per scommettere contro di me fra qualche anno. Probabilmente perderete i soldi perché ci azzecco raramente.

Nei prossimi 2-3 anni penso vedremo una transizione forzata verso la fase deflattiva, probabilmente innescata da uno di questi fattori:

1. **Crisi energetica**: I costi dell'energia diventeranno insostenibili
2. **Pressione climatica**: Governi costretti a limitare i consumi dei data center
3. **Saturazione del mercato**: I clienti smetteranno di pagare prezzi assurdi per AI marginalmente migliore
4. **Breakthrough tecnologico**: Qualcuno (forse Google o un player cinese) lancerà una soluzione drasticamente più efficiente

A quel punto, avremo una fase deflattiva accelerata dove l'efficientamento diventerà il nome del gioco. Chi avrà investito silenziosamente in chip specializzati, algoritmi ottimizzati e soluzioni quantum potrebbe dominare il mercato. Chi sarà rimasto attaccato alle GPU da mille watt si ritroverà con hardware obsoleto che vale quanto un Hummer in tempo di crisi petrolifera.

E poi? Poi forse inizierà il prossimo ciclo inflattivo, probabilmente centrato su qualche altra tecnologia che oggi nemmeno immaginiamo.

Ma ammetto che potrei aver capito tutto male.

## Storia che si ripete, ma più veloce

La cosa che mi affascina di più di tutto questo è che mi sembra stiamo ripetendo esattamente gli stessi errori delle bolle tecnologiche precedenti, ma **più velocemente**.

Negli anni '90 avevamo la bolla dot-com: tutti convinti che internet fosse magia, investimenti folli in aziende senza business model, crash inevitabile, poi ricostruzione su basi più solide. Ci sono voluti anni.

Oggi abbiamo quella che mi sembra la bolla AI: tutti convinti che l'intelligenza artificiale sia magia, investimenti folli in data center energivori, e probabilmente un crash inevitabile quando la realtà economica busserà alla porta. Ma stavolta tutto potrebbe succedere in una frazione del tempo.

La differenza è che l'AI, a differenza di molte startup dot-com, **funziona davvero**. Il problema non è la tecnologia, è il modo in cui stiamo artificialmente accelerando e prolungando i cicli economici per ragioni che hanno poco a che fare con l'efficienza economica.

È come guidare una Ferrari in prima marcia perché hai paura di scalare. Funziona, ma stai sprecando un sacco di carburante e prima o poi il motore si surriscalda.

O almeno, questa è la mia impressione. Magari è tutto normale e io non ci capisco niente.

---

La prossima volta che qualcuno vi paragona l'AI alla rivoluzione industriale, provate a fare loro una domanda semplice: "Quale rivoluzione industriale hai in mente? Quella che è durata decenni o quella che stiamo artificialmente accelerando per ragioni geopolitiche?"

Poi preparatevi a una lunga pausa imbarazzata. O magari vi risponderanno con qualcosa di molto intelligente e scoprirete che avevo torto su tutto. (Se succede, scrivetemi. Amo essere smentito quando imparo qualcosa di nuovo.)

Perché la verità è che non credo stiamo vivendo una rivoluzione industriale normale. Mi sembra che stiamo vivendo la prima rivoluzione industriale **compressa nel tempo e distorta dalla politica**. E francamente, nessuno sa davvero come andrà a finire.

L'unica cosa di cui sono ragionevolmente sicuro è che sarà molto più veloce, molto più caotico, e probabilmente molto più costoso di quello che chiunque stia prevedendo.

Benvenuti nel futuro. Allacciate le cinture. E se avete idee migliori delle mie, scrivetemi - anche solo per dirmi che sono un idiota. Almeno saprò che qualcuno ha letto fino alla fine.

---

*P.S. Se questo articolo vi è piaciuto e volete deprimervi ancora di più, nei prossimi giorni probabilmente scriverò qualcosa sui motivi per cui tutti i nostri smartphone diventeranno obsoleti prima di Natale. O forse no, dipende da quanta voglia avrò di farmi odiare.*